import penman
from pathlib import Path
from conllu import parse
from penman.graph import Graph
from penman.codec import PENMANCodec
from conllu import parse_incr
from conllu import parse


# def target_node(conll_gold, grammatical_function):
#     '''
#     get the head information according to the gold annotation
#     :param conll_gold: gold conllu file
#     :return: a list of tuples (the head id, the head token, and the grammatical function of the head)
#     '''
#     head_tokens =[]
#     source_tokens =[]
#     correct_parses =[]
#     wrong_parses=[]
#     for i, conll in enumerate(conll_gold):
#         tokens = [x for x in conll]
#         heads =[]
#         sources =[]
#         deprels = [x['deprel'] for x in conll]
#         if 'acl:relcl' in deprels:
#             for token in conll:
#                 if token['deprel'] == 'acl:relcl': #find the relative clauses first
#                     head_id = int(token['head'])
#                     head_info = [x for x in tokens if x['id'] == head_id][0]
#                     head_xpos = head_info['xpos']
#                     if head_xpos not in ['WDT', 'WP', 'WRB']:
#                         head_token = head_info['form']
#                         if head_info['deps']:
#                             head_rel = [x[0] for x in head_info['deps'] if x[1] == token['id']]
#                         else:
#                             head_rel=[]
#                         if head_rel == []:
#                             head_rel = [grammatical_function[0]]
#                         if head_rel[0] in grammatical_function or ' '.join(grammatical_function) in head_rel[0] or grammatical_function[0] in head_rel[0]:
#                             head = (head_id, head_token)
#                             heads.append(head)
#                             sources.append((token['id'], token['form']))
#         else:
#             heads.append('error dependency')
#             wrong_parses.append(i)
#         if not heads:
#             heads.append('error dependency')
#             wrong_parses.append(i)
#         if 'error dependency' not in heads:
#             correct_parses.append(i)
#         head_tokens.append(heads)
#         source_tokens.append(sources)
#     print(len(wrong_parses))
#     print(len(correct_parses))
#     return head_tokens, source_tokens, correct_parses

def target_node(conll_gold, grammatical_function):
    '''
    get the head information according to the gold annotation
    :param conll_gold: gold conllu file
    :return: a list of tuples (the head id, the head token, and the grammatical function of the head)
    '''

    head_tokens = []
    source_tokens = []

    for i, conll in enumerate(conll_gold):
        tokens = [x for x in conll]
        tokens_split_space = [x['form'] for x in tokens]
        deprels = ''.join([x['deprel'] for x in tokens])
        heads = []
        sources = []
        if 'acl:relcl' in deprels:
            for k, token in enumerate(conll):
                if token['deprel'] == 'acl:relcl':  # find the relative clauses first
                    head_id = int(token['head'])
                    head_info = [x for x in tokens if x['id'] == head_id][0]
                    # deps = head_info['deps']
                    # if deps:
                    #     deps = ' '.join([x[0] for x in deps])
                    # else:
                    deps = grammatical_function[0]

                    if deps and grammatical_function[0] in deps:
                        head_token = head_info['form']
                        # if head_info['deps']:
                        #     head_rel = [x[0] for x in head_info['deps'] if x[1] == token['id']]
                        # if head_rel == []:
                        head_rel = [grammatical_function[0]]

                        if head_rel[0] in grammatical_function or ' '.join(grammatical_function) in head_rel[0] or \
                                grammatical_function[0] in head_rel[0]:
                            real_head_token = [x for x in tokens_split_space if head_token in x]
                            real_head_token = sorted(real_head_token,
                                                     key=lambda x: tokens_split_space.index(x) - head_id)
                            head_id = tokens_split_space.index(real_head_token[0])
                            head = (head_id, head_token)
                            print(i, head)
                            heads.append(head)
                            token_form = token['form']
                            real_token_token = [x for x in tokens_split_space if token_form in x]
                            real_token_token = sorted(real_token_token,
                                                      key=lambda x: tokens_split_space.index(x) - token['id'])

                            sources.append((tokens_split_space.index(real_token_token[0]), token_form))
        else:
            heads.append('dependency error')
            sources.append('dependency error')
        if heads == []:  # we filter out unqualified sentences that contains for example csubj instead of nsubj
            heads.append('dependency error')
        # if i==1:
        #     print(tokens_split_space)
        #     print(heads)
        #     print(sources)
        head_tokens.append(heads)

        source_tokens.append(sources)
    filtered_out = [i for i, x in enumerate(head_tokens) if x != ['dependency error']]
    return head_tokens, source_tokens, set(filtered_out)


def find_node(alignment, head_id, source_id):
    '''
    to find the node that represents the head token
    :param alignment: the alignment document
    :param head_id: the head_id returned from the target_node function
    :return:
    '''
    print(len(alignment))
    print(len(head_id))
    assert len(alignment) == len(head_id)
    head_nodes = []
    source_nodes = []
    for i, alignment_element in enumerate(alignment):
        sentence = alignment_element.split('nodealignment=')[0].strip().split('\n')
        node_token_alignment = alignment_element.split('nodealignment=')[1][1:-2].strip().split(', ')
        head_node = []
        source_node = []
        token_id = [x.split('@@')[0] for x in node_token_alignment]
        token_node = [x.split('@@')[-1] for x in node_token_alignment]
        for j, token in enumerate(sentence):
            for h in head_id[i]:
                # print(h)
                if h != 'error dependency':

                    if h[1] in token.split('\t')[1]:

                        head_id[i].remove(h)
                        nodes = [y for x, y in zip(token_id, token_node) if int(x) - 1 == j]  # find the target node
                        for x in nodes:
                            head_node.append(x)

            for k in source_id[i]:
                if k[1] in token.split('\t')[1]:

                    source_id[i].remove(k)
                    s_nodes = [y for x, y in zip(token_id, token_node) if int(x) - 1 == j]  # find the target node
                    for x in s_nodes:
                        source_node.append(x)
        head_nodes.append(head_node)
        source_nodes.append(source_node)

    return head_nodes, source_nodes


def find_reentrancies(pred_amr, head_nodes, source_nodes, correct_parses):
    reentrancies = []
    reentrancies_general = []
    codec = PENMANCodec()
    head_num = []
    source_num = []
    reen_num = []
    for i, x in enumerate(pred_amr):
        drs_triple = codec.decode(x).triples
        label_node_pair = {x[0]: x[2].replace('\"', '') for x in drs_triple if x[1] == ':instance'}
        to_node = [x[2] for x in drs_triple if x[1] != ':instance' and x[2].startswith('u_')]  # only works for amparser
        # to_node_name = [label_node_pair[x[2]] for x in drs_triple if x[1]!=':instance' and x[2].startswith('u_')]
        if len(to_node) != len(set(to_node)) and i in correct_parses:
            reentrancies_general.append(drs_triple)
            reentrancy_node = list(set([label_node_pair[x] for x in to_node if to_node.count(x) > 1]))
            reentrancy_to_node = [x for x in to_node if to_node.count(x) > 1]
            reentrancy_from_node = [label_node_pair[x[0]] for x in drs_triple if x[2] in reentrancy_to_node]
            # print(reentrancy_node) #the node that has renentrancies
            # print(head_nodes[i]) # the real node that should have reentrancies
            overlap = [x for x in reentrancy_node if x in head_nodes[i]]
            from_source = [x for x in reentrancy_from_node if x in source_nodes[i]]
            if overlap and from_source:
                reentrancies.append(drs_triple)
                reen_num.append(i)
        if head_nodes[i]:
            head_num.append(i)
        if source_nodes[i]:
            source_num.append(i)
    correct_parses = set(correct_parses)
    reentrency_perc = round(len(reentrancies) / len(correct_parses) * 100, 1)
    print(
        f'Num of amrs:\t{len(correct_parses)}\nnum of examples that receive reentrancies:\t{len(reentrancies_general)}\n'
        f'num of examples in which the head node receives reentrancies:\t {reentrency_perc} ({len(reentrancies)}/{len(correct_parses)})\n'
        f'num of head tokens {len(set(head_num))}\n'
        f'num of source tokens {len(set(source_num))}')
    print(head_num)
    print(source_num)
    print(reen_num)
    print(len(set([x for x in source_num if x in head_num])))
    print(len(set(reen_num)))
    perc = len(set([x for x in source_num if x in head_num]))
    p = round((len(set(reen_num)) / perc) * 100, 1)
    print(f'{p} ({len(set(reen_num))}/{perc})')


if __name__ == '__main__':
    pred_amr = Path('parserOut.txt').read_text().strip().split('\n\n')
    alignment = Path('amr3_sc_alignment.txt').read_text().strip().split('sentence=')[1:]
    conll_gold = Path('amr/sc.conllu').read_text().strip().split('\n\n')
    conll_g = []
    for sent in conll_gold:
        sent = '\n'.join([x for x in sent.split('\n') if '-' not in x.split('\t')[0]])
        conll_g.append(parse(sent)[0])
    head_id, pred_id, correct_parses = target_node(conll_g, ['nsubj'])
    head_nodes, source_nodes = find_node(alignment, head_id, pred_id)
    find_reentrancies(pred_amr, head_nodes, source_nodes, correct_parses)
